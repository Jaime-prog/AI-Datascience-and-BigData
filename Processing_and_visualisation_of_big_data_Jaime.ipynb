{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 678
        },
        "id": "Evv7c66tF3BW",
        "outputId": "d7e318ee-13fc-4e56-b877-31f679ddf56e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\u001b[33m\r0% [Waiting for headers] [Waiting for headers] [1 InRelease 0 B/3,626 B 0%] [Co\u001b[0m\u001b[33m\r0% [Waiting for headers] [Waiting for headers] [Connecting to ppa.launchpadcont\u001b[0m\r                                                                               \rGet:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [557 kB]\n",
            "Get:5 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,184 kB]\n",
            "Get:6 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:7 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,012 kB]\n",
            "Get:8 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [1,384 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy/main Sources [2,231 kB]\n",
            "Get:13 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy/main amd64 Packages [1,145 kB]\n",
            "Hit:14 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,453 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,278 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1,410 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [32.6 kB]\n",
            "Fetched 12.0 MB in 11s (1,089 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "22 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425344 sha256=428553a7b8f866f32bf05672e50a682861c0736747be97f251d3130c88261ed3\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/spark-3.4.1-bin-hadoop3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "#Bibliotecas para poder trabajar con Spark\n",
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.4.1//spark-3.4.1-bin-hadoop3.tgz\n",
        "!tar xf spark-3.4.1-bin-hadoop3.tgz\n",
        "#Configuración de Spark con Python\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "\n",
        "#Estableciendo variable de entorno\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.1-bin-hadoop3\"\n",
        "\n",
        "#Buscando e inicializando la instalación de Spark\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/gdrive\")\n",
        "!pwd\n",
        "#put your own path in google drive\n",
        "%cd \"/content/gdrive/MyDrive/AI/Big_Data/archive_amazon\"\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8KEtxHuHuW1",
        "outputId": "249c60ea-e1a9-4c53-da0d-76b00d85b6aa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content\n",
            "/content/gdrive/MyDrive/AI/Big_Data/archive_amazon\n",
            "amazon_review_polarity_csv.tgz\ttest.csv  train.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName('AmazonReviews').getOrCreate()"
      ],
      "metadata": {
        "id": "RZC9Tx3tHu3n"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = spark.read.csv('train.csv', header=False, inferSchema=True)\n",
        "train_df = train_df.withColumnRenamed('_c0', 'polarity').withColumnRenamed('_c1', 'title').withColumnRenamed('_c2', 'text')\n",
        "\n",
        "test_df = spark.read.csv('test.csv', header=False, inferSchema=True)\n",
        "test_df = test_df.withColumnRenamed('_c0', 'polarity').withColumnRenamed('_c1', 'title').withColumnRenamed('_c2', 'text')"
      ],
      "metadata": {
        "id": "ubiQWFVTHvKZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# polarity - 1 for negative and 2 for positive\n",
        "train_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKqN0tGebKnk",
        "outputId": "e339caff-a4f5-4f0b-f0e8-6fce2d268b05"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------------------+--------------------+\n",
            "|polarity|               title|                text|\n",
            "+--------+--------------------+--------------------+\n",
            "|       2|Stuning even for ...|This sound track ...|\n",
            "|       2|The best soundtra...|I'm reading a lot...|\n",
            "|       2|            Amazing!|\"This soundtrack ...|\n",
            "|       2|Excellent Soundtrack|I truly like this...|\n",
            "|       2|Remember, Pull Yo...|If you've played ...|\n",
            "|       2|an absolute maste...|I am quite sure a...|\n",
            "|       1|        Buyer beware|\"This is a self-p...|\n",
            "|       2|      Glorious story|I loved Whisper o...|\n",
            "|       2|    A FIVE STAR BOOK|I just finished r...|\n",
            "|       2|Whispers of the W...|This was a easy t...|\n",
            "|       1|          The Worst!|A complete waste ...|\n",
            "|       2|          Great book|This was a great ...|\n",
            "|       2|          Great Read|I thought this bo...|\n",
            "|       1|           Oh please|I guess you have ...|\n",
            "|       1|Awful beyond belief!|\"I feel I have to...|\n",
            "|       1|Don't try to fool...|It's glaringly ob...|\n",
            "|       2|A romantic zen ba...|\"When you hear fo...|\n",
            "|       2|Fashionable Compr...|After I had a DVT...|\n",
            "|       2|Jobst UltraSheer ...|Excellent product...|\n",
            "|       1|sizes recomended ...|sizes are much sm...|\n",
            "+--------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aiFecTycidF",
        "outputId": "529bd759-4983-4a79-db0a-d06596c5443d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- polarity: integer (nullable = true)\n",
            " |-- title: string (nullable = true)\n",
            " |-- text: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* polaridad es una columna entera que puede contener valores nulos. Este indica la polaridad del sentimiento (positivo/negativo) de cada opinión.\n",
        "*  title y text son strings."
      ],
      "metadata": {
        "id": "3v-dUIVBdcLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.describe().show()\n",
        "test_df.describe().show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CX6obsypoo_7",
        "outputId": "0dd1df74-7e0c-4a51-817b-6ba95e40a755"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------+--------------------+--------------------+\n",
            "|summary|          polarity|               title|                text|\n",
            "+-------+------------------+--------------------+--------------------+\n",
            "|  count|           3600000|             3599952|             3599987|\n",
            "|   mean|               1.5|                 NaN|                null|\n",
            "| stddev|0.5000000694444585|                 NaN|                null|\n",
            "|    min|                 1|\u0010The Worst Thing ...|\u0003this is the best...|\n",
            "|    max|                 2|         ♦ LOVE IT ♦|…were Marvin and ...|\n",
            "+-------+------------------+--------------------+--------------------+\n",
            "\n",
            "+-------+-----------------+--------------------+--------------------+\n",
            "|summary|         polarity|               title|                text|\n",
            "+-------+-----------------+--------------------+--------------------+\n",
            "|  count|           400000|              399996|              399999|\n",
            "|   mean|              1.5|6.600378819412851E28|                10.0|\n",
            "| stddev|0.500000625001173|3.733738097285509E29|                null|\n",
            "|    min|                1|                   !| \"\" Judge for you...|\n",
            "|    max|                2|Ótimo livro. Mere...|à part le single ...|\n",
            "+-------+-----------------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_df.dropna()\n",
        "test_df = test_df.dropna()\n"
      ],
      "metadata": {
        "id": "9oFzaWiXo1mE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import count\n",
        "\n",
        "train_df.groupBy('polarity').agg(count('*').alias('count')).show()\n",
        "test_df.groupBy('polarity').agg(count('*').alias('count')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "joaKbwE2eSZ5",
        "outputId": "8af0c3b7-9345-406f-ff51-0f0b37193ff1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-------+\n",
            "|polarity|  count|\n",
            "+--------+-------+\n",
            "|       1|1799969|\n",
            "|       2|1799970|\n",
            "+--------+-------+\n",
            "\n",
            "+--------+------+\n",
            "|polarity| count|\n",
            "+--------+------+\n",
            "|       1|199998|\n",
            "|       2|199997|\n",
            "+--------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when\n",
        "\n",
        "train_df = train_df.withColumn('label', when(train_df['polarity'] == 1, 0).otherwise(1))\n",
        "test_df = test_df.withColumn('label', when(test_df['polarity'] == 1, 0).otherwise(1))"
      ],
      "metadata": {
        "id": "PaBALFkbfJmk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujuaXT9dfKuk",
        "outputId": "bfe1301b-af84-4ecb-ee33-cbe79fc33a1a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------------------+--------------------+-----+\n",
            "|polarity|               title|                text|label|\n",
            "+--------+--------------------+--------------------+-----+\n",
            "|       2|            Great CD|\"My lovely Pat ha...|    1|\n",
            "|       2|One of the best g...|Despite the fact ...|    1|\n",
            "|       1|Batteries died wi...|I bought this cha...|    0|\n",
            "|       2|works fine, but M...|Check out Maha En...|    1|\n",
            "|       2|Great for the non...|Reviewed quite a ...|    1|\n",
            "|       1|DVD Player crappe...|I also began havi...|    0|\n",
            "|       1|      Incorrect Disc|I love the style ...|    0|\n",
            "|       1|DVD menu select p...|I cannot scroll t...|    0|\n",
            "|       2|Unique Weird Orie...|\"Exotic tales of ...|    1|\n",
            "|       1|\"Not an \"\"ultimat...|Firstly,I enjoyed...|    0|\n",
            "|       2|Great book for tr...|I currently live ...|    1|\n",
            "|       1|                Not!|If you want to li...|    0|\n",
            "|       1|     A complete Bust|\"This game requir...|    0|\n",
            "|       2|TRULY MADE A DIFF...|I have been using...|    1|\n",
            "|       1|didn't run off of...|Was hoping that t...|    0|\n",
            "|       1|          Don't buy!|First of all, the...|    0|\n",
            "|       2|Simple, Durable, ...|This is an AWESOM...|    1|\n",
            "|       2|Review of Kelly C...|For the price of ...|    1|\n",
            "|       2|SOY UN APASIONADO...|Y ESTE LIBRO ESTÁ...|    1|\n",
            "|       2|Some of the best ...|This is an excell...|    1|\n",
            "+--------+--------------------+--------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#train_df = train_df.drop('polarity')\n",
        "#test_df = test_df.drop('polarity')"
      ],
      "metadata": {
        "id": "HpBTTbkNfa5j"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Processing the data\n",
        "\n",
        "from pyspark.sql.functions import regexp_replace, lower, split\n",
        "\n",
        "train_df = train_df.withColumn('title', regexp_replace(train_df['title'], '[^a-zA-Z0-9\\\\s]', ''))\n",
        "train_df = train_df.withColumn('title', lower(train_df['title']))\n",
        "#train_df = train_df.withColumn('title', split(train_df['title'], '\\\\s+'))\n",
        "\n",
        "train_df = train_df.withColumn('text', regexp_replace(train_df['text'], '[^a-zA-Z0-9\\\\s]', ''))\n",
        "train_df = train_df.withColumn('text', lower(train_df['text']))\n",
        "#train_df = train_df.withColumn('text', split(train_df['text'], '\\\\s+'))\n",
        "\n",
        "test_df = test_df.withColumn('title', regexp_replace(test_df['title'], '[^a-zA-Z0-9\\\\s]', ''))\n",
        "test_df = test_df.withColumn('title', lower(test_df['title']))\n",
        "#test_df = test_df.withColumn('title', split(test_df['title'], '\\\\s+'))\n",
        "\n",
        "test_df = test_df.withColumn('text', regexp_replace(test_df['text'], '[^a-zA-Z0-9\\\\s]', ''))\n",
        "test_df = test_df.withColumn('text', lower(test_df['text']))\n",
        "#test_df = test_df.withColumn('text', split(test_df['text'], '\\\\s+'))"
      ],
      "metadata": {
        "id": "pIdG7ETlHwni"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting only necessary columns\n",
        "\n",
        "model_train_df = train_df.select('text', 'label')\n",
        "model_test_df = test_df.select('text', 'label')"
      ],
      "metadata": {
        "id": "6i_UR4mGkSHd"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "tokenizer = Tokenizer(inputCol='text', outputCol='words')\n",
        "\n",
        "model_train_df = tokenizer.transform(model_train_df)\n",
        "model_test_df = tokenizer.transform(model_test_df)"
      ],
      "metadata": {
        "id": "HTlBHf1nHw5W"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we then convert the words column into an embed (feature vector)\n",
        "\n",
        "hashingTF = HashingTF(inputCol='words', outputCol='rawFeatures')\n",
        "\n",
        "model_train_df = hashingTF.transform(model_train_df)\n",
        "model_test_df = hashingTF.transform(model_test_df)"
      ],
      "metadata": {
        "id": "S2UPDZqyhihx"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idf = IDF(inputCol='rawFeatures', outputCol='features')\n",
        "\n",
        "idfModel = idf.fit(model_train_df)\n",
        "\n",
        "train_df_lr = idfModel.transform(model_train_df)\n",
        "test_df_lr = idfModel.transform(model_test_df)"
      ],
      "metadata": {
        "id": "hroIy15Kk-B_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = LogisticRegression(featuresCol='features', labelCol='label')\n",
        "\n",
        "lrModel = lr.fit(train_df_lr)"
      ],
      "metadata": {
        "id": "HjmYloEjmHE8"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = lrModel.transform(test_df_lr)"
      ],
      "metadata": {
        "id": "AhGE_LTl0Vsm"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol='rawPrediction', labelCol='label')\n",
        "\n",
        "auc = evaluator.evaluate(predictions)\n",
        "\n",
        "print('AUC:', auc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPtb8vFc0V-u",
        "outputId": "19675727-2882-441c-e88b-6ce656376ee0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC: 0.9190643603961509\n"
          ]
        }
      ]
    }
  ]
}